{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu21bMmAGXi_"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ2kQkvmGe8l",
        "outputId": "f0360817-08da-44b1-ec79-e294ca93374a"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhlok8FRbDir"
      },
      "source": [
        "#Importing NLTK libraries...\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2mvAFPRbjMS"
      },
      "source": [
        "#Reading file...\n",
        "import json\n",
        "with open('intents.json', 'r') as file:\n",
        "  intents = json.load(file)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKSIUrR5dWAh"
      },
      "source": [
        "#init classes...\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = [\"?\",\"!\",\".\"]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwoiO9QDd9Ms"
      },
      "source": [
        "#Tokenization...\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    tokenized_pattern = word_tokenize(pattern)\n",
        "    #extending words...basically creating corpus...\n",
        "    words.extend(tokenized_pattern)\n",
        "    #creating docs like (['how','are','you'],'greeting')\n",
        "    documents.append((tokenized_pattern, intent['tag']))\n",
        "    #basically creating a list of tags called classes....\n",
        "    if(intent['tag'] not in classes):\n",
        "      classes.append(intent['tag'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoYWFzjeg9Bt"
      },
      "source": [
        "#Lemmatizing and cleaning corpus...\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_words]\n",
        "\n",
        "words=sorted(list(set(words)))\n",
        "\n",
        "classes = sorted(list(set(classes)))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtnfk4d1jIAt"
      },
      "source": [
        "# creating pickle file for corpus ie words and for tags ie classes\n",
        "pickle.dump(words, open('words.pkl','wb'))\n",
        "pickle.dump(classes, open('classes.pkl','wb'))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsRe8bq1k4M2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}